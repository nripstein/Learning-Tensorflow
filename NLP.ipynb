{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:05:58.727244Z",
     "start_time": "2023-06-19T16:05:52.835756Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf # pip install tensorflow-macos\n",
    "import os\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The typical architecture of a Recurrent Neural Network (RNN)\n",
    "- The premise of an RNN is simple: use information from the past to help you with the future (this is where the term recurrent comes from). In other words, take an input (X) and compute an output (y) based on all previous inputs.\n",
    "\n",
    "When an RNN looks at a sequence of text (already in numerical form), the patterns it learns are continually updated based on the order of the sequence.\n",
    "\n",
    "For a simple example, take two sentences:\n",
    "\n",
    "1. Massive earthquake last week, no?\n",
    "2. No massive earthquake last week.\n",
    "\n",
    "Both contain exactly the same words but have different meaning. The order of the words determines the meaning (one could argue punctuation marks also dictate the meaning but for simplicity sake, let's stay focused on the words).\n",
    "\n",
    "### actual architecture\n",
    "1. input layer\n",
    "2. text vectorization layer\n",
    "3. embedding\n",
    "4. RNN cell(s)\n",
    "5. Hidden activation\n",
    "6. pooling layer (sometimes, usually for Conv1D models)\n",
    "7. fully connected layer\n",
    "8. output layer\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing a notebook for our first NLP with TensorFlow project\n",
    "[data from kaggle](https://www.kaggle.com/competitions/nlp-getting-started/leaderboard)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from helper_functions import create_tensorboard_callback, unzip_data, plot_loss_curves, compare_historys"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.031773Z",
     "start_time": "2023-06-19T16:05:58.728280Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "zip_path = \"nlp_getting_started.zip\"\n",
    "if not os.path.isfile(zip_path):\n",
    "    os.chdir(\"data\")\n",
    "unzip_data(zip_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.043776Z",
     "start_time": "2023-06-19T16:06:00.032393Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Becoming one with the data and visualizing a text dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# we can use pandas because the data isn't too big\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# shuffle training data\n",
    "train_df = train_df.sample(frac=1, random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.070701Z",
     "start_time": "2023-06-19T16:06:00.044698Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "         id               keyword                  location   \n3228   4632  emergency%20services   Sydney, New South Wales  \\\n3706   5271                  fear                       NaN   \n6957   9982               tsunami         Land Of The Kings   \n2887   4149                 drown                       NaN   \n7464  10680                wounds  cody, austin follows ?*?   \n\n                                                   text  target  \n3228  Goulburn man Henry Van Bilsen missing: Emergen...       1  \n3706  The things we fear most in organizations--fluc...       0  \n6957                            @tsunami_esh ?? hey Esh       0  \n2887  @POTUS you until you drown by water entering t...       0  \n7464  Crawling in my skin\\nThese wounds they will no...       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3228</th>\n      <td>4632</td>\n      <td>emergency%20services</td>\n      <td>Sydney, New South Wales</td>\n      <td>Goulburn man Henry Van Bilsen missing: Emergen...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3706</th>\n      <td>5271</td>\n      <td>fear</td>\n      <td>NaN</td>\n      <td>The things we fear most in organizations--fluc...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6957</th>\n      <td>9982</td>\n      <td>tsunami</td>\n      <td>Land Of The Kings</td>\n      <td>@tsunami_esh ?? hey Esh</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2887</th>\n      <td>4149</td>\n      <td>drown</td>\n      <td>NaN</td>\n      <td>@POTUS you until you drown by water entering t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7464</th>\n      <td>10680</td>\n      <td>wounds</td>\n      <td>cody, austin follows ?*?</td>\n      <td>Crawling in my skin\\nThese wounds they will no...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.080720Z",
     "start_time": "2023-06-19T16:06:00.075273Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'Goulburn man Henry Van Bilsen missing: Emergency services are searching for a Goulburn man who disappeared from his\\x89Û_ http://t.co/z99pKJzTRp'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0][\"text\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.086667Z",
     "start_time": "2023-06-19T16:06:00.082185Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "   id keyword location                                               text\n0   0     NaN      NaN                 Just happened a terrible car crash\n1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data frame looks the same but without targets\n",
    "test_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.113757Z",
     "start_time": "2023-06-19T16:06:00.088658Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "target\n0    4342\n1    3271\nName: count, dtype: int64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"target\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.115254Z",
     "start_time": "2023-06-19T16:06:00.092029Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613 3263\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df), len(test_df))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.159801Z",
     "start_time": "2023-06-19T16:06:00.100977Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Militants attack police post in Udhampur; 2 SPOs injured\n",
      "\n",
      "Suspected militants Thursday  attacked a police post in... http://t.co/1o0j9FCPBi\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "I hear the mumbling i hear the cackling i got em scared shook panicking\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "8/6/2015@2:09 PM: TRAFFIC ACCIDENT NO INJURY at 2781 WILLIS FOREMAN RD http://t.co/VCkIT6EDEv\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Evacuation order lifted for town of Roosevelt Wash. though residents warned to be ready to leave quickly http://t.co/Na0ptN0dTr\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "You messed up my feeling like a hurricane damaged this broken home\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's visualize some random samples\n",
    "import random\n",
    "random_index = random.randint(0, len(train_df)-5)\n",
    "for row in train_df[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"---\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.181265Z",
     "start_time": "2023-06-19T16:06:00.107505Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Splitting data into training and validation sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df[\"text\"].to_numpy(),\n",
    "                                                                            train_df[\"target\"].to_numpy(),\n",
    "                                                                            test_size=0.1,\n",
    "                                                                            random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.236356Z",
     "start_time": "2023-06-19T16:06:00.112376Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6851 6851 762 762\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentences), len(train_labels), len(val_sentences), len(val_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.236785Z",
     "start_time": "2023-06-19T16:06:00.140841Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Rly tragedy in MP: Some live to recount horror: \\x89ÛÏWhen I saw coaches of my train plunging into water I called my daughters and said t...',\n       'A river of lava in the sky this evening! It was indeed a beautiful sunset sky tonight. (8-4-15) http://t.co/17EGMlNi80',\n       'Los Angeles Times: Arson suspect linked to 30 fires caught in Northern ... - http://t.co/xwMs1AWW8m #NewsInTweets http://t.co/TE2YeRugsi',\n       ...,\n       \"When you lowkey already know you're gonna drown in school this year :) http://t.co/aCMrm833zq\",\n       '@JamesMelville Some old testimony of weapons used to promote conflicts\\nTactics - corruption &amp; infiltration of groups\\nhttps://t.co/cyU8zxw1oH',\n       \"Just got evacuated from the movie theatre for an emergency. Saw people running from another they're.\"],\n      dtype=object)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:-10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.237052Z",
     "start_time": "2023-06-19T16:06:00.148446Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 0, 1, ..., 0, 0, 1])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:-10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.237552Z",
     "start_time": "2023-06-19T16:06:00.160014Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Converting text data to numbers using tokenisation and embeddings (overview)\n",
    "### Tokenization\n",
    "- A straight mapping from word or character or sub-word to a numerical value. There are three main levels of tokenization:\n",
    "1. Using word-level tokenization with the sentence \"I love TensorFlow\" might result in \"I\" being 0, \"love\" being 1 and \"TensorFlow\" being 2. In this case, every word in a sequence considered a single token.\n",
    "2. Character-level tokenization, such as converting the letters A-Z to values 1-26. In this case, every character in a sequence considered a single token.\n",
    "3. Sub-word tokenization is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, \"my favourite food is pineapple pizza\" might become \"my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za\". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple tokens.\n",
    "\n",
    "\n",
    "### Embeddings\n",
    "- An embedding is a representation of natural language which can be learned. Representation comes in the form of a feature vector. For example, the word \"dance\" could be represented by the 5-dimensional vector [-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]. It's important to note here, the size of the feature vector is tuneable. There are two ways to use embeddings:\n",
    "1. Create your own embedding - Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as tf.keras.layers.Embedding) and an embedding representation will be learned during model training.\n",
    "2. Reuse a pre-learned embedding - Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setting up a TensorFlow TextVectorization layer to convert text to numbers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.237648Z",
     "start_time": "2023-06-19T16:06:00.160330Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# these are the default parameters. this cell is just for demonstration\n",
    "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
    "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
    "                                    split=\"whitespace\", # how to split tokens\n",
    "                                    ngrams=None, # create groups of n-words?\n",
    "                                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
    "                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.256455Z",
     "start_time": "2023-06-19T16:06:00.162284Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "15"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find average number of tokens (words) in training Tweets\n",
    "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.258551Z",
     "start_time": "2023-06-19T16:06:00.202861Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Setup text vectorization with custom variables\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=max_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.267395Z",
     "start_time": "2023-06-19T16:06:00.208420Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mapping the TextVectorization layer to text data and turning it into numbers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 12:06:00.248538: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(train_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.410331Z",
     "start_time": "2023-06-19T16:06:00.214011Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\narray([[319,   3, 199,   4,  13, 699,   0,   0,   0,   0,   0,   0,   0,\n          0,   0]])>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sample sentence and tokenize it\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.429391Z",
     "start_time": "2023-06-19T16:06:00.411573Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "@oooureli @Abu_Baraa1 You mean like the tolerance you showed when sharing 'democracy' with the Iraqis? Wait you mutilated and bombed them.      \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\narray([[   1, 6264,   12, 1202,   25,    2, 7486,   12, 3418,   45, 4641,\n        3064,   14,    2,    1]])>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a random sentence from the training dataset and tokenize it\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nVectorized version:\")\n",
    "text_vectorizer([random_sentence])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.448203Z",
     "start_time": "2023-06-19T16:06:00.430542Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "Bottom 5 least common words: ['palmer', 'palm', 'palinfoen', 'palestinian\\x89Û', 'paleface']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
    "bottom_5_words = words_in_vocab[-5:] # least common tokens\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\")\n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")\n",
    "\n",
    "# UNK is unknown token which replaces uncommon words. increasing max tokens will reduce it's popularity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.448935Z",
     "start_time": "2023-06-19T16:06:00.436095Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating an Embedding layer to turn tokenised text into embedding vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "embedding = tf.keras.layers.Embedding(input_dim=max_vocab_length,  # set input shape\n",
    "                                      output_dim=128,  # a common starting point, multiples of 8 tend to run faster\n",
    "                                      input_length=max_length,  # how long is the input?\n",
    "                                      name=\"embedding_1\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.458882Z",
     "start_time": "2023-06-19T16:06:00.450447Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "@fadelurker @dalinthanelan &lt; right now.\n",
      "\n",
      "Even after two years there were still refugees camped just south of Redcliffe village and Aidan &gt;      \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\narray([[[ 0.00044354, -0.00959869, -0.01488503, ...,  0.0157225 ,\n         -0.01731045,  0.04999502],\n        [ 0.00044354, -0.00959869, -0.01488503, ...,  0.0157225 ,\n         -0.01731045,  0.04999502],\n        [ 0.03572508,  0.00295686, -0.01646795, ..., -0.04597085,\n          0.00974417, -0.02150788],\n        ...,\n        [ 0.0406287 ,  0.03047732, -0.0343025 , ...,  0.03056096,\n         -0.04152142,  0.03783656],\n        [ 0.00044354, -0.00959869, -0.01488503, ...,  0.0157225 ,\n         -0.01731045,  0.04999502],\n        [-0.01843027,  0.04141413, -0.0358417 , ...,  0.01633341,\n         -0.02188298,  0.00408796]]], dtype=float32)>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence from training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into numerical representation)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.516993Z",
     "start_time": "2023-06-19T16:06:00.455179Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(128,), dtype=float32, numpy=\narray([ 0.00044354, -0.00959869, -0.01488503,  0.01583595, -0.03460299,\n       -0.01922628,  0.03886281,  0.03049261,  0.02703375,  0.03125577,\n       -0.00640028,  0.04225099, -0.04081216, -0.01396431, -0.02797878,\n       -0.00030531, -0.03165816,  0.02005385, -0.02563877, -0.04627147,\n        0.01255215, -0.01807749,  0.04352308, -0.04396086, -0.00285302,\n        0.01801059,  0.02089453, -0.02251923,  0.03271903, -0.04593432,\n        0.04534021,  0.01232741,  0.01351071, -0.00348172, -0.00295175,\n        0.01398399, -0.04538589,  0.03654433,  0.0401935 ,  0.01342661,\n        0.02652259, -0.00834689,  0.04488019,  0.04263305, -0.01620691,\n       -0.03888427, -0.00747744, -0.01589997,  0.0127697 , -0.04039965,\n        0.00297211,  0.02234465,  0.01354844, -0.03515201,  0.00236771,\n       -0.01941361,  0.0042451 , -0.03211303,  0.01056878,  0.01654197,\n       -0.02855355, -0.00332244, -0.03632966, -0.03077023, -0.00369436,\n       -0.00707566, -0.02659743, -0.0120948 ,  0.04943614,  0.00123626,\n       -0.03087481, -0.00372937, -0.00132525,  0.04827496, -0.03834536,\n        0.03710565, -0.021214  , -0.01131842, -0.01339864,  0.0219068 ,\n       -0.0317181 ,  0.00147084, -0.04092928,  0.00054431, -0.03393292,\n       -0.04293679,  0.04456509, -0.0183426 ,  0.03583744,  0.02865675,\n        0.03487456, -0.03574425,  0.00950282,  0.00323985,  0.02638496,\n        0.04618556, -0.0251058 ,  0.00241145, -0.04017676, -0.01832813,\n       -0.02586591,  0.02167474, -0.02180682,  0.00496869,  0.00573482,\n        0.03419032, -0.01664885, -0.01081491,  0.01707159,  0.03341324,\n        0.01989045,  0.01257339,  0.02060959, -0.0496264 ,  0.01551045,\n       -0.01332706, -0.02620391, -0.04656223,  0.00638485, -0.04882344,\n       -0.03853918,  0.04177115, -0.02008035,  0.00322092,  0.03942195,\n        0.0157225 , -0.01731045,  0.04999502], dtype=float32)>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TensorShape([128])"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'@fadelurker @dalinthanelan &lt; right now.\\n\\nEven after two years there were still refugees camped just south of Redcliffe village and Aidan &gt;'"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check out a single token's embedding\n",
    "display(sample_embed[0][0])\n",
    "display(sample_embed[0][0].shape)\n",
    "display(random_sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.525489Z",
     "start_time": "2023-06-19T16:06:00.473967Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# the various modelling experiments we're going to run\n",
    "- [Model 0: Sklearn Naive Bayes (baseline)](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- Model 1: Feed-forward neural network (dense model)\n",
    "- Model 2: LSTM model\n",
    "- Model 3: GRU model\n",
    "- Model 4: Bidirectional-LSTM model\n",
    "- Model 5: 1D Convolutional Neural Network\n",
    "- Model 6: TensorFlow Hub Pretrained Feature Extractor\n",
    "- Model 7: Same as model 6 with 10% of training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 0: Building a baseline model to try and improve upon [Sklearn Naive Bayes (baseline)](https://scikit-learn.org/stable/modules/naive_bayes.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling pipeline (pipeline is a bit like tf.keras.models.Sequential()\n",
    "model_0 = Pipeline([\n",
    "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
    "                    (\"clf\", MultinomialNB()) # model the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.592298Z",
     "start_time": "2023-06-19T16:06:00.480974Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our baseline model achieves an accuracy of: 78.22%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model:\n",
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.602949Z",
     "start_time": "2023-06-19T16:06:00.597942Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.639794Z",
     "start_time": "2023-06-19T16:06:00.609665Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a function to track and evaluate our model's results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Function to evaluate: accuracy, precision, recall, f1-score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "    only works for binary classification\n",
    "    Args:\n",
    "    -----\n",
    "    y_true = true labels in the form of a 1D array\n",
    "    y_pred = predicted labels in the form of a 1D array\n",
    "\n",
    "    Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "    \"\"\"\n",
    "    # Calculate model accuracy\n",
    "    model_accuracy = accuracy_score(y_true, y_pred)\n",
    "    # Calculate model precision, recall and f1 score using \"weighted\" average\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    model_results = {\"accuracy\": model_accuracy,\n",
    "                  \"precision\": model_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "    return model_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.649350Z",
     "start_time": "2023-06-19T16:06:00.617109Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "{'accuracy': 0.7821522309711286,\n 'precision': 0.7922635210264124,\n 'recall': 0.7821522309711286,\n 'f1': 0.7730378142460546}"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline results\n",
    "baseline_results = calculate_results(y_true=val_labels, y_pred=baseline_preds)\n",
    "baseline_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.650149Z",
     "start_time": "2023-06-19T16:06:00.621288Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 1: Building, fitting and evaluating our first deep model (feed forward) on text data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# Create tensorboard callback (need to create a new one for each model)\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "# Create directory to save TensorBoard logs\n",
    "SAVE_DIR = \"model_logs\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.650372Z",
     "start_time": "2023-06-19T16:06:00.639986Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs) # turn the input text into numbers\n",
    "x = embedding(x)  # create embedding of numberized inputs\n",
    "x = layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\") # construct the model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.700606Z",
     "start_time": "2023-06-19T16:06:00.640489Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.709368Z",
     "start_time": "2023-06-19T16:06:00.662638Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:00.709778Z",
     "start_time": "2023-06-19T16:06:00.674686Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/simple_dense_model/20230619-120600\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.6096 - accuracy: 0.6907 - val_loss: 0.5432 - val_accuracy: 0.7507\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4390 - accuracy: 0.8221 - val_loss: 0.4917 - val_accuracy: 0.7887\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.3452 - accuracy: 0.8619 - val_loss: 0.4848 - val_accuracy: 0.7927\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.2821 - accuracy: 0.8904 - val_loss: 0.4973 - val_accuracy: 0.7979\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.2344 - accuracy: 0.9134 - val_loss: 0.5179 - val_accuracy: 0.7808\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_1_history = model_1.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
    "                                                                     experiment_name=\"simple_dense_model\")])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:06.662837Z",
     "start_time": "2023-06-19T16:06:00.683550Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 631us/step - loss: 0.5179 - accuracy: 0.7808\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.5178927779197693, 0.7808399200439453]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the results\n",
    "model_1.evaluate(val_sentences, val_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:06.700687Z",
     "start_time": "2023-06-19T16:06:06.664338Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 481us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0.9963774 ],\n       [0.9327017 ],\n       [0.8655261 ],\n       [0.00449484],\n       [0.28397885],\n       [0.30158865],\n       [0.84945637],\n       [0.20643485],\n       [0.11917296],\n       [0.07175671]], dtype=float32)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_pred_probs[:10] # only print out the first 10 prediction probabilities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:06.778194Z",
     "start_time": "2023-06-19T16:06:06.703534Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(20,), dtype=float32, numpy=\narray([1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n       0., 1., 1.], dtype=float32)>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn prediction probabilities into single-dimension tensor of floats\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) # squeeze removes single dimensions\n",
    "model_1_preds[:20]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:06.803830Z",
     "start_time": "2023-06-19T16:06:06.779249Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "{'accuracy': 0.7808398950131233,\n 'precision': 0.7798665327287134,\n 'recall': 0.7808398950131233,\n 'f1': 0.7779809505888746}"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model_1 metrics\n",
    "model_1_results = calculate_results(y_true=val_labels,\n",
    "                                    y_pred=model_1_preds)\n",
    "model_1_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:06.805011Z",
     "start_time": "2023-06-19T16:06:06.784191Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "array([False, False, False,  True])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is our simple Keras model better than our baseline model?\n",
    "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))\n",
    "# nope"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:06.805171Z",
     "start_time": "2023-06-19T16:06:06.788912Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualizing our model's learned word embeddings with [TensorFlow's projector tool](https://projector.tensorflow.org/)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# redoing this from above for practice\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab), words_in_vocab[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:06.812280Z",
     "start_time": "2023-06-19T16:06:06.797518Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:06.851614Z",
     "start_time": "2023-06-19T16:06:06.811725Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# weight matrix of embedding layer\n",
    "# (these are the numerical patterns between the text in the training dataset the model has learned)\n",
    "embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
    "print(embed_weights.shape) # same size as vocab size and embedding_dim (each word is a embedding_dim size vector)\n",
    "print(3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:06.852081Z",
     "start_time": "2023-06-19T16:06:06.824352Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Create output writers\n",
    "out_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
    "out_m = io.open(\"embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "# Write embedding vectors and words to file\n",
    "for num, word in enumerate(words_in_vocab):\n",
    "  if num == 0:\n",
    "     continue # skip padding token\n",
    "  vec = embed_weights[num]\n",
    "  out_m.write(word + \"\\n\") # write words to file\n",
    "  out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # write corresponding word vector to file\n",
    "out_v.close()\n",
    "out_m.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:07.234550Z",
     "start_time": "2023-06-19T16:06:06.827139Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# High-level overview of Recurrent Neural Networks (RNNs) + where to learn more\n",
    "- RNNs are useful for sequence data\n",
    "- The premise of an RNN is simple: use information from the past to help you with the future (this is where the term recurrent comes from). In other words, take an input (X) and compute an output (y) based on all previous inputs\n",
    "### types of RNNs\n",
    "- Long short-term memory cells (LSTMs).\n",
    "- Gated recurrent units (GRUs).\n",
    "- Bidirectional RNN's (passes forward and backward along a sequence, left to right and right to left).\n",
    "### types of problems RNNs can be used in\n",
    "- One to one: one input, one output, such as image classification.\n",
    "- One to many: one input, many outputs, such as image captioning (image input, a sequence of text as caption output).\n",
    "- Many to one: many inputs, one outputs, such as text classification (classifying a Tweet as real diaster or not real diaster).\n",
    "- Many to many: many inputs, many outputs, such as machine translation (translating English to Spanish) or speech to text (audio wave as input, text as output)\n",
    "### Resources\n",
    "- [MIT Deep Learning Lecture on Recurrent Neural Networks](https://youtu.be/SEnXr6v2ifU)  - explains the background of recurrent neural networks and introduces LSTMs.\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) - demonstrates the power of RNN's with examples generating various sequences.\n",
    "- [Understanding LSTMs by Chris Olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) - an in-depth (and technical) look at the mechanics of the LSTM cell, possibly the most popular RNN building block."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 2: Building, fitting and evaluating our first TensorFlow RNN model (LSTM)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The reason we use a new embedding layer for each model is since the embedding layer is a learned representation of words (as numbers), if we were to use the same embedding layer (embedding_1) for each model, we'd be mixing what one model learned with the next. And because we want to compare our models later on, starting them with their own embedding layer each time is a better idea."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 15, 128)\n",
      "(None, 64)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "model_2_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_2\")\n",
    "\n",
    "\n",
    "# Create LSTM model\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_2_embedding(x)\n",
    "print(x.shape)\n",
    "# x = layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n",
    "x = layers.LSTM(64)(x) # return vector for whole sequence\n",
    "print(x.shape)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer on top of output of LSTM cell\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:07.383774Z",
     "start_time": "2023-06-19T16:06:07.234742Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:07.384299Z",
     "start_time": "2023-06-19T16:06:07.370445Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_2_experiment/20230619-120607\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 4s 15ms/step - loss: 0.5094 - accuracy: 0.7453 - val_loss: 0.2982 - val_accuracy: 0.8788\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.3115 - accuracy: 0.8739 - val_loss: 0.1809 - val_accuracy: 0.9356\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.2085 - accuracy: 0.9229 - val_loss: 0.1408 - val_accuracy: 0.9603\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.1484 - accuracy: 0.9477 - val_loss: 0.1091 - val_accuracy: 0.9673\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.1107 - accuracy: 0.9571 - val_loss: 0.0833 - val_accuracy: 0.9724\n"
     ]
    }
   ],
   "source": [
    "model_2_history = model_2.fit(x=train_sentences,\n",
    "                              y=train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(train_sentences, train_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"model_2_experiment\")])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:22.265821Z",
     "start_time": "2023-06-19T16:06:07.386135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,329,473\n",
      "Trainable params: 1,329,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:22.273863Z",
     "start_time": "2023-06-19T16:06:22.266689Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n",
      "(762, 1) [[0.99987584]\n",
      " [0.99068624]\n",
      " [0.99947435]\n",
      " [0.00554314]\n",
      " [0.17340145]\n",
      " [0.0267905 ]\n",
      " [0.9998617 ]\n",
      " [0.15163232]\n",
      " [0.10999082]\n",
      " [0.00434084]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation dataset\n",
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "print(model_2_pred_probs.shape, model_2_pred_probs[:10]) # view the first 10\n",
    "\n",
    "# Round out predictions and reduce to 1-dimensional array\n",
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_preds[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:22.518479Z",
     "start_time": "2023-06-19T16:06:22.276654Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "{'accuracy': 0.7414698162729659,\n 'precision': 0.7411148256584495,\n 'recall': 0.7414698162729659,\n 'f1': 0.7412807067637125}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'accuracy': 0.7821522309711286,\n 'precision': 0.7922635210264124,\n 'recall': 0.7821522309711286,\n 'f1': 0.7730378142460546}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate LSTM model results\n",
    "model_2_results = calculate_results(y_true=val_labels,\n",
    "                                    y_pred=model_2_preds)\n",
    "display(model_2_results)  # worse than baseline\n",
    "display(baseline_results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:22.524239Z",
     "start_time": "2023-06-19T16:06:22.519873Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 3: Building, fitting and evaluating a GRU-cell powered\n",
    "- The GRU cell has similar features to an LSTM cell but has less parameters\n",
    "- again, we'll use this architecture: Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.GRU(64, return_sequences=False)(x) # you need return sequences true to stack reccurant layers\n",
    "# x = layers.LSTM(64)(x, return_sequences=True)\n",
    "# x = layers.GRU(64)(x)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_3 = tf.keras.Model(inputs, outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:22.610661Z",
     "start_time": "2023-06-19T16:06:22.526519Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,317,313\n",
      "Trainable params: 1,317,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:22.620453Z",
     "start_time": "2023-06-19T16:06:22.611065Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_3_GRU/20230619-120622\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 3s 10ms/step - loss: 0.2253 - accuracy: 0.9096 - val_loss: 0.7000 - val_accuracy: 0.7690\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.1483 - accuracy: 0.9489 - val_loss: 0.7760 - val_accuracy: 0.7677\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.1231 - accuracy: 0.9555 - val_loss: 0.7528 - val_accuracy: 0.7493\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.1062 - accuracy: 0.9623 - val_loss: 0.7637 - val_accuracy: 0.7546\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.0948 - accuracy: 0.9670 - val_loss: 0.8678 - val_accuracy: 0.7520\n"
     ]
    }
   ],
   "source": [
    "model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_3_history = model_3.fit(train_sentences,\n",
    "            train_labels,\n",
    "            epochs=5,\n",
    "            validation_data=(val_sentences, val_labels),\n",
    "            callbacks=[create_tensorboard_callback(SAVE_DIR, \"model_3_GRU\")])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:33.831520Z",
     "start_time": "2023-06-19T16:06:22.623913Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# make predictions with model 3\n",
    "model_3_pred_probs = model_3.predict(val_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:34.032283Z",
     "start_time": "2023-06-19T16:06:33.833610Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# Convert prediction probabilities to prediction classes\n",
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:34.032788Z",
     "start_time": "2023-06-19T16:06:34.027423Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "{'accuracy': 0.7519685039370079,\n 'precision': 0.7512241125208886,\n 'recall': 0.7519685039370079,\n 'f1': 0.7515319432391477}"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcuate model_3 results\n",
    "model_3_results = calculate_results(y_true=val_labels,\n",
    "                                    y_pred=model_3_preds)\n",
    "model_3_results  # still not as good as baseline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:34.033474Z",
     "start_time": "2023-06-19T16:06:34.027688Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 4: Building, fitting and evaluating a bidirectional RNN model\n",
    "- A standard RNN will process a sequence from left to right, where as a bidirectional RNN will process the sequence from left to right and then again from right to left.\n",
    "- Intuitively, this can be thought of as if you were reading a sentence for the first time in the normal fashion (left to right) but for some reason it didn't make sense so you traverse back through the words and go back over them again (right to left).\n",
    "- In practice, many sequence models often see and improvement in performance when using bidirectional RNN's.\n",
    "- However, this improvement in performance often comes at the cost of longer training times and increased model parameters (since the model goes left to right and right to left, the number of trainable parameters doubles)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "# x = text_vectorizer(inputs)\n",
    "# x = embedding(x)\n",
    "# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "# x = layers.Bidirectional(layers.GRU(64, return_sequences=False))(x)\n",
    "# outputs = layers.Dense(1, activation=\"sigmoid\")\n",
    "#\n",
    "# model_4 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# his\n",
    "# inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "# x = text_vectorizer(inputs)\n",
    "# x = model_4_embedding(x)\n",
    "# # x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # stacking RNN layers requires return_sequences=True\n",
    "# x = layers.Bidirectional(layers.LSTM(64))(x) # bidirectional goes both ways so has double the parameters of a regular LSTM layer\n",
    "# outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "# model_4 = tf.keras.Model(inputs, outputs, name=\"model_4_Bidirectional\")\n",
    "\n",
    "# his modified\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # stacking RNN layers requires return_sequences=True\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x) # bidirectional goes both ways so has double the parameters of a regular LSTM layer\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_4 = tf.keras.Model(inputs, outputs, name=\"model_4_Bidirectional\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:34.276546Z",
     "start_time": "2023-06-19T16:06:34.034010Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_Bidirectional\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,378,945\n",
      "Trainable params: 1,378,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_4.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:34.287165Z",
     "start_time": "2023-06-19T16:06:34.276790Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_4_bidirectional/20230619-120634\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 4s 12ms/step - loss: 0.1314 - accuracy: 0.9583 - val_loss: 0.8018 - val_accuracy: 0.7480\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.0797 - accuracy: 0.9730 - val_loss: 1.0278 - val_accuracy: 0.7349\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.0721 - accuracy: 0.9746 - val_loss: 1.0587 - val_accuracy: 0.7323\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.0662 - accuracy: 0.9761 - val_loss: 1.0660 - val_accuracy: 0.7336\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.0621 - accuracy: 0.9753 - val_loss: 1.2472 - val_accuracy: 0.7493\n"
     ]
    }
   ],
   "source": [
    "model_4.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "model_4_history = model_4.fit(train_sentences,\n",
    "            train_labels,\n",
    "            epochs=5,\n",
    "            validation_data=(val_sentences, val_labels),\n",
    "            callbacks=[create_tensorboard_callback(SAVE_DIR, \"model_4_bidirectional\")])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:47.532601Z",
     "start_time": "2023-06-19T16:06:34.290065Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'accuracy': 0.7493438320209974,\n 'precision': 0.7482324836878728,\n 'recall': 0.7493438320209974,\n 'f1': 0.74862942348961}"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_pred_probs = model_4.predict(val_sentences)\n",
    "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
    "model_4_results = calculate_results(val_labels, model_4_preds)\n",
    "model_4_results  # still worse than baseline, also worse than uni-directional in this case"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:47.827548Z",
     "start_time": "2023-06-19T16:06:47.534465Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conv1D neural networks for text and sequences\n",
    "let's look at what it does"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test out the embedding, 1D convolutional and max pooling\n",
    "embedding_test = embedding(text_vectorizer([\"this is a test sentence\"])) # turn target sentence into embedding\n",
    "conv_1d = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\") # convolve over target sequence 5 words at a time\n",
    "conv_1d_output = conv_1d(embedding_test) # pass embedding through 1D convolutional layer\n",
    "max_pool = layers.GlobalMaxPool1D()\n",
    "max_pool_output = max_pool(conv_1d_output) # get the most important features\n",
    "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:47.841701Z",
     "start_time": "2023-06-19T16:06:47.830034Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The embedding has an output shape dimension of the parameters we set it to (input_length=15 and output_dim=128).\n",
    "\n",
    "The 1-dimensional convolutional layer has an output which has been compressed inline with its parameters. And the same goes for the max pooling layer output.\n",
    "\n",
    "Our text starts out as a string but gets converted to a feature vector of length 64 through various transformation steps (from tokenization to embedding to 1-dimensional convolution to max pool).\n",
    "\n",
    "Let's take a peak at what each of these transformations looks like."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n array([[[ 2.47879792e-02, -3.76283042e-02, -4.78201099e-02, ...,\n          -9.77362916e-02, -2.47036237e-02,  1.04545252e-02],\n         [ 4.81648147e-02,  1.05584515e-02, -1.94007379e-03, ...,\n          -3.20963301e-02,  2.00950690e-02,  3.29664797e-02],\n         [-2.23150402e-02, -4.24621534e-03,  3.29113677e-02, ...,\n          -2.85842549e-02, -4.58227694e-02, -1.35427881e-02],\n         ...,\n         [-2.03758404e-02, -5.34846149e-02, -5.64603251e-05, ...,\n           1.46892974e-02, -3.05895861e-02,  1.38282441e-02],\n         [-2.03758404e-02, -5.34846149e-02, -5.64603251e-05, ...,\n           1.46892974e-02, -3.05895861e-02,  1.38282441e-02],\n         [-2.03758404e-02, -5.34846149e-02, -5.64603251e-05, ...,\n           1.46892974e-02, -3.05895861e-02,  1.38282441e-02]]],\n       dtype=float32)>,\n <tf.Tensor: shape=(1, 11, 32), dtype=float32, numpy=\n array([[[6.11400232e-02, 4.75114733e-02, 0.00000000e+00, 1.16474926e-03,\n          2.92749852e-02, 0.00000000e+00, 0.00000000e+00, 1.01708546e-02,\n          2.87779514e-02, 3.47702913e-02, 5.52408397e-03, 4.60318178e-02,\n          0.00000000e+00, 0.00000000e+00, 3.86786312e-02, 0.00000000e+00,\n          2.81273089e-02, 3.43655832e-02, 0.00000000e+00, 0.00000000e+00,\n          9.44700558e-03, 1.02601118e-01, 1.08971454e-01, 0.00000000e+00,\n          0.00000000e+00, 0.00000000e+00, 5.94953224e-02, 0.00000000e+00,\n          0.00000000e+00, 1.91867352e-04, 0.00000000e+00, 0.00000000e+00],\n         [3.07988022e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n          0.00000000e+00, 1.20858094e-02, 0.00000000e+00, 0.00000000e+00,\n          2.93448605e-02, 7.25055486e-03, 8.16312581e-02, 0.00000000e+00,\n          0.00000000e+00, 0.00000000e+00, 1.47942211e-02, 0.00000000e+00,\n          0.00000000e+00, 1.04483962e-02, 6.30294085e-02, 3.17628086e-02,\n          7.80331343e-02, 0.00000000e+00, 2.87139043e-03, 2.45264694e-02,\n          0.00000000e+00, 2.88682375e-02, 0.00000000e+00, 6.34204224e-03,\n          0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n         [1.34270806e-02, 1.17795803e-02, 0.00000000e+00, 0.00000000e+00,\n          1.20067783e-02, 0.00000000e+00, 4.93548363e-02, 3.59044597e-03,\n          0.00000000e+00, 0.00000000e+00, 1.47735197e-02, 8.87993257e-03,\n          0.00000000e+00, 4.61755432e-02, 5.24623431e-02, 8.33494775e-03,\n          2.18427777e-02, 0.00000000e+00, 3.96643132e-02, 2.82277148e-02,\n          0.00000000e+00, 0.00000000e+00, 1.60128959e-02, 0.00000000e+00,\n          0.00000000e+00, 4.19863909e-02, 0.00000000e+00, 0.00000000e+00,\n          0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n         [4.90117669e-02, 6.20838925e-02, 1.45688765e-02, 0.00000000e+00,\n          8.54667649e-03, 5.46880588e-02, 7.82136694e-02, 2.62549594e-02,\n          0.00000000e+00, 2.66700797e-03, 0.00000000e+00, 0.00000000e+00,\n          0.00000000e+00, 0.00000000e+00, 1.29761333e-02, 1.81492791e-02,\n          0.00000000e+00, 2.57291291e-02, 2.17648931e-02, 1.03009865e-04,\n          8.13007914e-03, 9.00226925e-03, 2.02872548e-02, 0.00000000e+00,\n          0.00000000e+00, 0.00000000e+00, 3.35097313e-02, 2.08491813e-02,\n          0.00000000e+00, 1.68400630e-02, 0.00000000e+00, 0.00000000e+00],\n         [5.29905856e-02, 4.51157056e-02, 0.00000000e+00, 0.00000000e+00,\n          4.56949249e-02, 3.28727439e-02, 6.78990856e-02, 0.00000000e+00,\n          0.00000000e+00, 0.00000000e+00, 1.70123577e-03, 0.00000000e+00,\n          0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.12043706e-02,\n          4.10845410e-03, 0.00000000e+00, 4.82656211e-02, 0.00000000e+00,\n          0.00000000e+00, 2.86712311e-02, 5.52970096e-02, 4.99164686e-03,\n          0.00000000e+00, 0.00000000e+00, 4.62670289e-02, 0.00000000e+00,\n          2.23909952e-02, 4.16276976e-02, 0.00000000e+00, 0.00000000e+00],\n         [4.71827313e-02, 4.45677713e-02, 0.00000000e+00, 0.00000000e+00,\n          2.54634358e-02, 4.44264039e-02, 2.84436420e-02, 0.00000000e+00,\n          2.15077959e-03, 0.00000000e+00, 2.23926753e-02, 0.00000000e+00,\n          0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.18244377e-02,\n          0.00000000e+00, 0.00000000e+00, 4.65194620e-02, 1.70352235e-02,\n          0.00000000e+00, 0.00000000e+00, 4.50225621e-02, 2.34734248e-02,\n          0.00000000e+00, 0.00000000e+00, 4.68411297e-02, 0.00000000e+00,\n          1.34513853e-02, 2.58407649e-03, 0.00000000e+00, 0.00000000e+00],\n         [4.71827313e-02, 4.45677713e-02, 0.00000000e+00, 0.00000000e+00,\n          2.54634358e-02, 4.44264039e-02, 2.84436420e-02, 0.00000000e+00,\n          2.15077959e-03, 0.00000000e+00, 2.23926753e-02, 0.00000000e+00,\n          0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.18244377e-02,\n          0.00000000e+00, 0.00000000e+00, 4.65194620e-02, 1.70352235e-02,\n          0.00000000e+00, 0.00000000e+00, 4.50225621e-02, 2.34734248e-02,\n          0.00000000e+00, 0.00000000e+00, 4.68411297e-02, 0.00000000e+00,\n          1.34513853e-02, 2.58407649e-03, 0.00000000e+00, 0.00000000e+00],\n         [4.71827313e-02, 4.45677713e-02, 0.00000000e+00, 0.00000000e+00,\n          2.54634358e-02, 4.44264039e-02, 2.84436420e-02, 0.00000000e+00,\n          2.15077959e-03, 0.00000000e+00, 2.23926753e-02, 0.00000000e+00,\n          0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.18244377e-02,\n          0.00000000e+00, 0.00000000e+00, 4.65194620e-02, 1.70352235e-02,\n          0.00000000e+00, 0.00000000e+00, 4.50225621e-02, 2.34734248e-02,\n          0.00000000e+00, 0.00000000e+00, 4.68411297e-02, 0.00000000e+00,\n          1.34513853e-02, 2.58407649e-03, 0.00000000e+00, 0.00000000e+00],\n         [4.71827313e-02, 4.45677713e-02, 0.00000000e+00, 0.00000000e+00,\n          2.54634358e-02, 4.44264039e-02, 2.84436420e-02, 0.00000000e+00,\n          2.15077959e-03, 0.00000000e+00, 2.23926753e-02, 0.00000000e+00,\n          0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.18244377e-02,\n          0.00000000e+00, 0.00000000e+00, 4.65194620e-02, 1.70352235e-02,\n          0.00000000e+00, 0.00000000e+00, 4.50225621e-02, 2.34734248e-02,\n          0.00000000e+00, 0.00000000e+00, 4.68411297e-02, 0.00000000e+00,\n          1.34513853e-02, 2.58407649e-03, 0.00000000e+00, 0.00000000e+00],\n         [4.71827313e-02, 4.45677713e-02, 0.00000000e+00, 0.00000000e+00,\n          2.54634358e-02, 4.44264039e-02, 2.84436420e-02, 0.00000000e+00,\n          2.15077959e-03, 0.00000000e+00, 2.23926753e-02, 0.00000000e+00,\n          0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.18244377e-02,\n          0.00000000e+00, 0.00000000e+00, 4.65194620e-02, 1.70352235e-02,\n          0.00000000e+00, 0.00000000e+00, 4.50225621e-02, 2.34734248e-02,\n          0.00000000e+00, 0.00000000e+00, 4.68411297e-02, 0.00000000e+00,\n          1.34513853e-02, 2.58407649e-03, 0.00000000e+00, 0.00000000e+00],\n         [4.71827313e-02, 4.45677713e-02, 0.00000000e+00, 0.00000000e+00,\n          2.54634358e-02, 4.44264039e-02, 2.84436420e-02, 0.00000000e+00,\n          2.15077959e-03, 0.00000000e+00, 2.23926753e-02, 0.00000000e+00,\n          0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.18244377e-02,\n          0.00000000e+00, 0.00000000e+00, 4.65194620e-02, 1.70352235e-02,\n          0.00000000e+00, 0.00000000e+00, 4.50225621e-02, 2.34734248e-02,\n          0.00000000e+00, 0.00000000e+00, 4.68411297e-02, 0.00000000e+00,\n          1.34513853e-02, 2.58407649e-03, 0.00000000e+00, 0.00000000e+00]]],\n       dtype=float32)>,\n <tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n array([[0.06114002, 0.06208389, 0.01456888, 0.00116475, 0.04569492,\n         0.05468806, 0.07821367, 0.02625496, 0.02934486, 0.03477029,\n         0.08163126, 0.04603182, 0.        , 0.04617554, 0.05246234,\n         0.04182444, 0.02812731, 0.03436558, 0.06302941, 0.03176281,\n         0.07803313, 0.10260112, 0.10897145, 0.02452647, 0.        ,\n         0.04198639, 0.05949532, 0.02084918, 0.022391  , 0.0416277 ,\n         0.        , 0.        ]], dtype=float32)>)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the outputs of each layer\n",
    "embedding_test[:1], conv_1d_output[:1], max_pool_output[:1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:06:47.861395Z",
     "start_time": "2023-06-19T16:06:47.842251Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Making the conv1d model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_Conv1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 11, 32)            20512     \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Glo  (None, 32)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,300,545\n",
      "Trainable params: 1,300,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "model_5_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_5\")\n",
    "\n",
    "# Create 1-dimensional convolutional layer to model sequences\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_5_embedding(x)\n",
    "x = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_5 = tf.keras.Model(inputs, outputs, name=\"model_5_Conv1D\")\n",
    "\n",
    "# Compile Conv1D model\n",
    "model_5.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_5.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:07:58.904774Z",
     "start_time": "2023-06-19T16:07:58.846775Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/Conv1D/20230619-120811\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.5609 - accuracy: 0.7181 - val_loss: 0.4811 - val_accuracy: 0.7861\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3387 - accuracy: 0.8612 - val_loss: 0.5054 - val_accuracy: 0.7664\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2040 - accuracy: 0.9253 - val_loss: 0.5893 - val_accuracy: 0.7782\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1279 - accuracy: 0.9597 - val_loss: 0.7047 - val_accuracy: 0.7493\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.0899 - accuracy: 0.9720 - val_loss: 0.7530 - val_accuracy: 0.7415\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_5_history = model_5.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR,\n",
    "                                                                     \"Conv1D\")])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:08:17.415276Z",
     "start_time": "2023-06-19T16:08:11.122081Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 680us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0.99992543],\n       [0.9976896 ],\n       [0.986874  ],\n       [0.00708563],\n       [0.00330716],\n       [0.14532353],\n       [0.9840111 ],\n       [0.14718454],\n       [0.16802555],\n       [0.00865445]], dtype=float32)"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_pred_probs = model_5.predict(val_sentences)\n",
    "model_5_pred_probs[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:09:56.335587Z",
     "start_time": "2023-06-19T16:09:56.228395Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)>"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert model_5 prediction probabilities to labels\n",
    "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
    "model_5_preds[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:10:02.650018Z",
     "start_time": "2023-06-19T16:10:02.644806Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "{'accuracy': 0.7414698162729659,\n 'precision': 0.7398429301909109,\n 'recall': 0.7414698162729659,\n 'f1': 0.7402801500377935}"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model_5 evaluation metrics\n",
    "model_5_results = calculate_results(y_true=val_labels,\n",
    "                                    y_pred=model_5_preds)\n",
    "model_5_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:10:11.763245Z",
     "start_time": "2023-06-19T16:10:11.757448Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using TensorFlow Hub for pretrained word embeddings (transfer learning for NLP)\n",
    "- We'll use [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.01157025  0.02485909  0.0287805  -0.01271501  0.03971541  0.08827759\n",
      "  0.02680986  0.05589836 -0.01068732 -0.00597294  0.00639323 -0.01819521\n",
      "  0.00030815  0.09105891  0.05874644 -0.03180626  0.01512474 -0.05162928\n",
      "  0.00991368 -0.06865346 -0.04209306  0.0267898   0.03011009  0.00321064\n",
      " -0.00337969 -0.0478736   0.02266718 -0.00985928 -0.04063616 -0.01292095\n",
      " -0.04666382  0.05630299 -0.03949255  0.00517684  0.02495827 -0.07014439\n",
      "  0.02871509  0.04947678 -0.00633972 -0.08960193  0.0280712  -0.00808366\n",
      " -0.01360598  0.0599865  -0.10361788 -0.05195374  0.00232957 -0.0233253\n",
      " -0.03758107  0.0332773 ], shape=(50,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Example of pretrained embedding with universal sentence encoder - https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "import tensorflow_hub as hub\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\") # load Universal Sentence Encoder\n",
    "embed_samples = embed([sample_sentence,\n",
    "                      \"When you call the universal sentence encoder on a sentence, it turns it into numbers.\"])\n",
    "\n",
    "print(embed_samples[0][:50])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:30:52.115585Z",
     "start_time": "2023-06-19T16:30:25.174881Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([512])"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each sentence has been encoded into a 512 dimension vector\n",
    "embed_samples[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:30:52.120738Z",
     "start_time": "2023-06-19T16:30:52.117601Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 6: Building, training and evaluating a transfer learning model for NLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# We can use this encoding layer in place of our text_vectorizer and embedding layer\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[], # shape of inputs coming to our model\n",
    "                                        dtype=tf.string, # data type of inputs coming to the USE layer\n",
    "                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n",
    "                                        name=\"USE\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:35:43.296752Z",
     "start_time": "2023-06-19T16:35:40.362184Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we've got the USE as a Keras layer, we can use it in a Keras Sequential model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6_USE\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,830,721\n",
      "Trainable params: 32,897\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model 6 (transfer learning) using the Sequential API\n",
    "model_6 = tf.keras.Sequential([\n",
    "  sentence_encoder_layer, # take in sentences and then encode them into an embedding\n",
    "  layers.Dense(64, activation=\"relu\"),\n",
    "  layers.Dense(1, activation=\"sigmoid\")\n",
    "], name=\"model_6_USE\")\n",
    "\n",
    "# Compile model\n",
    "model_6.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_6.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:36:30.152443Z",
     "start_time": "2023-06-19T16:36:29.917215Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/tf_hub_sentence_encoder/20230619-123642\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 2s 5ms/step - loss: 0.4965 - accuracy: 0.7878 - val_loss: 0.4688 - val_accuracy: 0.7874\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.4144 - accuracy: 0.8164 - val_loss: 0.4620 - val_accuracy: 0.7953\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.3993 - accuracy: 0.8213 - val_loss: 0.4576 - val_accuracy: 0.7927\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.3912 - accuracy: 0.8289 - val_loss: 0.4610 - val_accuracy: 0.7979\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3853 - accuracy: 0.8320 - val_loss: 0.4587 - val_accuracy: 0.7992\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on top of pretrained embeddings\n",
    "model_6_history = model_6.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR,\n",
    "                                                                     \"tf_hub_sentence_encoder\")])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:36:47.684528Z",
     "start_time": "2023-06-19T16:36:42.636405Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0.963828  ],\n       [0.857974  ],\n       [0.81991976],\n       [0.03819581],\n       [0.05435634],\n       [0.48794934],\n       [0.8404128 ],\n       [0.15077174],\n       [0.50047183],\n       [0.60435563]], dtype=float32)"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with USE TF Hub model\n",
    "model_6_pred_probs = model_6.predict(val_sentences)\n",
    "model_6_pred_probs[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:37:37.886367Z",
     "start_time": "2023-06-19T16:37:37.608821Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 0., 0., 0., 1., 0., 1., 1.], dtype=float32)>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert prediction probabilities to labels\n",
    "model_6_preds = tf.squeeze(tf.round(model_6_pred_probs))\n",
    "model_6_preds[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:37:43.738688Z",
     "start_time": "2023-06-19T16:37:43.734183Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "{'accuracy': 0.7992125984251969,\n 'precision': 0.7985566013932281,\n 'recall': 0.7992125984251969,\n 'f1': 0.7969086902314371}"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model 6 performance metrics\n",
    "model_6_results = calculate_results(val_labels, model_6_preds)\n",
    "model_6_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T16:37:50.677896Z",
     "start_time": "2023-06-19T16:37:50.672516Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing subsets of data for model 7 (same as model 6 but 10% of data)\n",
    "- One of the benefits of using transfer learning methods, such as, the pretrained embeddings within the USE is the ability to get great results on a small amount of data (the USE paper even mentions this in the abstract).\n",
    "\n",
    "- To put this to the test, we're going to make a small subset of the training data (10%), train a model and evaluate it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
